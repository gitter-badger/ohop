{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75dbc7f",
   "metadata": {},
   "source": [
    "# Episode 16: Handmade Parsers\n",
    "\n",
    "Calculators are the wave of the past...long live calculators!\n",
    "\n",
    "To make matters even more old school, let's write a simple calculator language by hand!\n",
    "\n",
    "Grammar:\n",
    "\n",
    "```\n",
    "statements : EMPTY | statement statements\n",
    "statement  : (assign | expression) '\\n'\n",
    "assign     : ID '=' expression\n",
    "expression : atom (op expression)?\n",
    "atom       : ID | NUM\n",
    "op         : '+' | '-'\n",
    "```\n",
    "\n",
    "## Step 1: Lexical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f473f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "EMPTY = 0\n",
    "NL = 1\n",
    "ID = 2\n",
    "EQ = 3\n",
    "NUM = 4\n",
    "PLUS = 5\n",
    "MINUS = 6\n",
    "\n",
    "Token = typing.Tuple[int, str]\n",
    "\n",
    "def calc_lexer(source: str) -> typing.Iterator[Token]:\n",
    "    index = 0\n",
    "    while index < len(source):\n",
    "        char = source[index]\n",
    "        if char in ' \\t':\n",
    "            pass\n",
    "        elif char == '\\n':\n",
    "            yield (NL, char)\n",
    "        elif char.isalpha():\n",
    "            result = char\n",
    "            index += 1\n",
    "            if index < len(source):\n",
    "                char = source[index]\n",
    "                while index < len(source) and char.isalpha():\n",
    "                    result += char\n",
    "                    index += 1\n",
    "                    char = source[index]\n",
    "            yield (ID, result)\n",
    "            continue\n",
    "        elif char == '=':\n",
    "            yield (EQ, char)\n",
    "        elif char.isdigit():\n",
    "            result = char\n",
    "            index += 1\n",
    "            if index < len(source):\n",
    "                char = source[index]\n",
    "                while index < len(source) and char.isdigit():\n",
    "                    result += char\n",
    "                    index += 1\n",
    "                    char = source[index]\n",
    "            yield(NUM, result)\n",
    "            continue\n",
    "        elif char == '+':\n",
    "            yield (PLUS, char)\n",
    "        elif char == '-':\n",
    "            yield (MINUS, char)\n",
    "        else:\n",
    "            raise SyntaxError(f'Unexpected character in input string: {char}')\n",
    "        index += 1\n",
    "    yield (EMPTY, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21277a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'A'),\n",
       " (3, '='),\n",
       " (4, '2'),\n",
       " (1, '\\n'),\n",
       " (2, 'B'),\n",
       " (3, '='),\n",
       " (4, '3'),\n",
       " (1, '\\n'),\n",
       " (2, 'A'),\n",
       " (5, '+'),\n",
       " (2, 'B'),\n",
       " (0, '')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(calc_lexer('A = 2\\nB = 3\\nA + B'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e8422",
   "metadata": {},
   "source": [
    "## Step 2: Syntactic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbd21f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokens(typing.NamedTuple):\n",
    "    lookahead: typing.List[Token]\n",
    "    stream: typing.Iterator[Token]\n",
    "\n",
    "class Tree(typing.NamedTuple):\n",
    "    contents: typing.Union[str, Token]\n",
    "    children: typing.List['Tree']\n",
    "\n",
    "def peek(tokens: Tokens) -> Token:\n",
    "    next_token = next(tokens.stream)\n",
    "    tokens.lookahead.push(next_token)\n",
    "    return tokens.lookahead[-1]\n",
    "\n",
    "def next_token(tokens: Tokens) -> Token:\n",
    "    if len(tokens.lookahead) > 0:\n",
    "        return tokens.lookahead.pop()\n",
    "    return next(tokens.stream)\n",
    "\n",
    "def parse_statements(tokens: Tokens) -> Tree:\n",
    "    token = next_token(tokens)\n",
    "    if token[0] == EMPTY:\n",
    "        result = Tree(token, [])\n",
    "    else:\n",
    "        child_0 = parse_statement(tokens)\n",
    "        child_1 = parse_statements(tokens)\n",
    "        result = Tree('statements', [child_0, child_1])\n",
    "    return result\n",
    "\n",
    "def parse_statement(tokens: Tokens) -> Tree:\n",
    "    token = peek(tokens)\n",
    "    if token[0] == ID:\n",
    "        token = peek(tokens)\n",
    "        if token[0] == EQ:\n",
    "            child_result = parse_assign(tokens)\n",
    "        else:\n",
    "            child_result = parse_expression(tokens)\n",
    "    else:\n",
    "        child_result = parse_expression(tokens)\n",
    "    nl_token = next_token(tokens)\n",
    "    assert nl_token[0] == NL\n",
    "    return Tree('statement', [child_result, nl_token])\n",
    "\n",
    "def parse_assign(tokens: Tokens) -> Tree:\n",
    "    identifier = next_token(tokens)\n",
    "    eq = next_token(tokens)\n",
    "    assert eq[0] == EQ\n",
    "    rhs = parse_expression(tokens)\n",
    "    return Tree('assign', [identifier, eq, rhs])\n",
    "\n",
    "def parse_expression(tokens: Tokens) -> Tree:\n",
    "    atom = parse_atom(tokens)\n",
    "    token = peek(tokens)\n",
    "    if token[0] in (PLUS, MINUS):\n",
    "        op = parse_op(tokens)\n",
    "        rhs = parse_expression(tokens)\n",
    "        result = Tree('expression', [atom, op, rhs])\n",
    "    else:\n",
    "        result = atom\n",
    "    return result\n",
    "\n",
    "def parse_atom(tokens: Tokens) -> Tree:\n",
    "    token = next_token(tokens)\n",
    "    assert token[0] in (ID, NUM)\n",
    "    return Tree('atom', [Tree(token, [])])\n",
    "\n",
    "def parse_op(tokens: Tokens) -> Tree:\n",
    "    token = next_token(tokens)\n",
    "    assert token[0] in (PLUS, MINUS)\n",
    "    return Tree('op', [Tree(token, [])])\n",
    "\n",
    "def calc_parse(tokens: Token) -> Tree:\n",
    "    return parse_statements(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e8af2",
   "metadata": {},
   "source": [
    "## Step 3: Tie Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03ca7712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_frontend(source: str) -> Tree:\n",
    "    return calc_parse(Tokens([], calc_lexer(source)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cda37d31",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'push'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q7/ylhf40fn5kq9xrbfmsl61q4h0000gn/T/ipykernel_14949/522978481.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalc_frontend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1 + 3 - 5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/q7/ylhf40fn5kq9xrbfmsl61q4h0000gn/T/ipykernel_14949/1631294509.py\u001b[0m in \u001b[0;36mcalc_frontend\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_frontend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcalc_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalc_lexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/q7/ylhf40fn5kq9xrbfmsl61q4h0000gn/T/ipykernel_14949/2038403130.py\u001b[0m in \u001b[0;36mcalc_parse\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mToken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparse_statements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/q7/ylhf40fn5kq9xrbfmsl61q4h0000gn/T/ipykernel_14949/2038403130.py\u001b[0m in \u001b[0;36mparse_statements\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mchild_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mchild_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_statements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'statements'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchild_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/q7/ylhf40fn5kq9xrbfmsl61q4h0000gn/T/ipykernel_14949/2038403130.py\u001b[0m in \u001b[0;36mparse_statement\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mID\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/q7/ylhf40fn5kq9xrbfmsl61q4h0000gn/T/ipykernel_14949/2038403130.py\u001b[0m in \u001b[0;36mpeek\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mToken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnext_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookahead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookahead\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'push'"
     ]
    }
   ],
   "source": [
    "calc_frontend('1 + 3 - 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7092989f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
